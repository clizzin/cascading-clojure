HIGH LEVEL DESCRIPTION

project setup & build:

1. Get and install Leiningen http://github.com/technomancy/leiningen
2. In cascading-clojure home run: 
   $ lein clean
   $ lein deps; lein install

This should download all dependencies, and install cascading-clojure in your local repo.

PACKAGING AND MAIN CLASS

In your client code's project.clj, make sure to include your ":main my.namespace" 

you currently can not use lein uberjar.  you have to build the jar manually with "jar cfe main.namespace lib/ classes/" (your lein project's lib/ folder will have cascading and deps, and clojure jars).

your main class looks like:
 (ns mycompany.filename
  (:require [cascading.clojure.cascading :as c])
  (:require [cascading.clojure.makemain-utils :as m])
  (:gen-class))
  
(defn- -main [& args]
  (let [opts (assoc (m/parseArgs args) :main-class (class -main))]
    (if (:join opts) 
      (c/cascading-join opts)
      (c/cascading opts))))

Cascading-clojure uses apache commons cli to parse commandline arguments. A 
sample command looks like this: 
       hadoop jar build/jobs.jar \
       -in file:///input/dir/ \
       -out file:///output/dir/ \
       -wf my.namespace/my-fn

-in and -out are input and output paths, -wf is the fully-qualified name of the function that defines your workflow map.

Right now, it works on rows, where each row can be thought of as an item in the "collection".  row depends on your input format or scheme, (in cascading parlance) for example, if you use the default textline, each row is a line of text from the input files.

you to define read and write functions. Examples are identity, read-string, fns for reading json, etc. you can also define input and output fields.

DEFINING WORKFLOWS

sample code at: http://gist.github.com/183376

there are currently a few supported cascading constructs...
each, every, and groupby.

Each of these take in functions that's called per row, or in everygroup's case,
per group.

Workflows are defined as functions that return maps, these are called by the
internal wrapper code when you specify which workflow/function to call in the
commandline when you tell hadoop to run a job. (more on this later)

These workflow maps in general have a function for the operation, a reader
(which converts a row into something meaningful for your program), and a writer,
 (usually just print-string/pr-str). There are defaults defined for each of
these... of the cascading constructs take in additional fns. (for example,
groupby takes in a function to extract a group key out of the line of input)

The readers and writers are called per field of the output rows.

EACH

Each is like map in clojure, it takes in a function with an arity of the number 
of fields you specify, which defaults to 1, and calls it over each row.

You can multiple results per row that you process, and each row has to 
correspond to the input fields that you specify.


The output shape has to be a seq of seqs.
The inner seqs are rows, and since each call to Each can return multiple rows,
we need the outer seq.


The defaults are
:using identity :reader read-string :writer pr-str :inputFields ["line"]
:outputFields ["data"]


GROUPBY

GroupBy groups rows together, it is typically followed by an everyGroup. It
groups things by Fields/FIRST. The difference with each is that it takes in a
groupkey extractor function. This groupkey extractor has an arity of the num of
input fields. The return value is jsut anything that implements comparable.

Everything else is pretty similar to each, just don't forget that the first
field of the outputFields will be used as the key.


defaults:
:using identity :reader read-string :writer pr-str :groupby (fn [x] 1)
:inputFields ["line"] :outputFields ["key", "clojurecode"]



EVERYGROUP

This has to be preceded by a groupBy, as per cascading rules. 
(http://www.cascading.org/javadoc/cascading/pipe/Every.html)
EveryGroup is similar to reduce/fold, with an "init" function that will return
the initial value. This initial value has to be a seq that matches the
outputFields count. (since we're "collecting" these values per row we process)

the using function has to take in the accumulated row so far, and should output
a row. These rows should match the number of outputFields.


defaults:
:using (fn [acc next-line] [(str (first acc) next-line)]) :reader read-string
:writer pr-str :init (fn [] [""]) :inputFields ["line"] :outputFields ["data"]

GOTCHAS

cascading textline's fields are "offset" and "line". you have to name the
inputfields properly. Also when specifying multiple steps in the workflow, your
input and output fields have to line up for consecutive steps.